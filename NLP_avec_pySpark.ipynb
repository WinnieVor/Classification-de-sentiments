{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Winnie VORIHILALA <br>\n",
    "MS ESD 2019-2020 <br>\n",
    "INSA ROUEN <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce TP consiste à entraîner un algorithme de classification de sentiments à l'aide d'un Framework de calcul distribué de notre choix, en utilisant les jeux de données suivants :\n",
    "- train.json : contenant le dataset d'entraînement de l'algorithme\n",
    "- test.json : contenant le dataset de test de l'algorithme\n",
    "- noclass.json : sur lequel il faudra effectuer des prédictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> SOMMAIRE <br>\n",
    "    \n",
    "1- Contexte <br>\n",
    "<br>\n",
    "1.1 - Big Data <br>\n",
    "1.2 - Calculs parallèles vs calculs distribués : différence <br>\n",
    "1.3 - Calculs parallèles vs calculs distribués : avantages <br>\n",
    "1.4 - Hadoop MapReduce vs Apache Spark <br>\n",
    "<br>\n",
    "\n",
    "2- Code <br>\n",
    "<br>\n",
    "2.1 - Chargement des librairies <br>\n",
    "<br>\n",
    "2.2 - Configuration de l'enviromment pySpark <br>\n",
    "<br>\n",
    "2.3 - Chargement des données et visualisation <br>\n",
    "2.3.1 - Chargement des données <br>\n",
    "2.3.2 - Visualisation des données <br>\n",
    "2.3.2.1 - Visualisation du jeu de données train <br>\n",
    "2.3.2.2 - Visualisation du jeu de données test <br>\n",
    "2.3.2.3 - Visualisation du jeu de données noclass <br>\n",
    "2.3.2.4 - Analyse des jeux de données <br>\n",
    "\n",
    "2.4 - Pré-traitement des données <br>\n",
    "2.4.1 - Fonction de pre-processing <br>\n",
    "2.4.2 - 1ère méthode de pre processing : nltk <br>\n",
    "2.4.3 - 2ème méthode de pre processing : regexTokenizer et stopwordsRemover <br>\n",
    "\n",
    "2.5 - Extraction de features avec CountVector, TF IDF, word2vec et StringIndexer <br>\n",
    "\n",
    "2.6 - Classification de sentiments <br>\n",
    "2.6.1 -  Clasification avec NaivesBayes et Logistic Regression <br>\n",
    "2.6.2 - Prediction sur les données de test <br>\n",
    "2.6.2.1 - 1ère méthode : avec CountVectors <br>\n",
    "2.6.2.2 - 2ème méthode : avec TF-IDF <br>\n",
    "2.6.3 -  Cross validation <br>\n",
    "\n",
    "2.7 - Prediction sur le jeu de données noclass.json et sauveragarde du fichier <br>\n",
    "<br>\n",
    "3- Récapituatif des méthodes utilisées <br>\n",
    "<br>\n",
    "4- Distribution des calculs <br>\n",
    "4.1 - Map Reduce <br>\n",
    "4.2 - Cluster Spark <br>\n",
    "4.3 - RDD <br>\n",
    "4.4 - Calculs distribués sous forme de graphe avec les DAG <br>\n",
    "4.5 - Fonctionnement de spark : cycle de vie d'une application <br>\n",
    "4.6 - Distribution des calculs dans notre application Spark\n",
    "\n",
    "Sources\n",
    "<br>\n",
    "</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- CONTEXTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On parle de Big Data de manière générale lorsque :\n",
    "- les données sont trop grosses pour être stockées en RAM\n",
    "- la quantité de données excède la faculté d'une machine à les stocker et les analyser en un temps acceptable.\n",
    "<br>\n",
    "La solution à ce problème est de paralléliser les calculs sur plusieurs machines différentes. D'où les notions de calculs parallèles et de calculs distribués."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - Calculs parallèles vs calculs distribués : différence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculs parallèles : lors d'un calcul réalisé en parallèle, différents threads d'exécutions sont exécutés en même temps et partagent une mémoire commune qui leur permettent de se synchroniser entre eux. <br>\n",
    "- Calculs distribués : Dans le calcul distribué, les nœuds sur lequels les calculs sont exécutés sont distants, autonomes et ne partagent pas de ressources ; la communication entre les nœuds s'effectue grâce à l'envoi de messages, au sein d'un cluster. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 - Calculs parallèles vs calculs distribués : avantages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculs distribués : <br>\n",
    "=> ce modèle de calcul résout un certain nombre de problèmes : par exemple, le passage à l'échelle s'effectue de manière <font color=\"red\" > horizontale </font> c'est à dire qu'il suffit d'ajouter des nœuds au cluster pour augmenter sa capacité de calcul. <br>\n",
    "=> permet une plus grande tolérance aux pannes (lorsqu'un nœud du cluster subit une panne, il suffit d'affecter la tâche qu'il était en train de traiter à un autre nœud, alors que dans le modèle parallèle la machine sur laquelle le calcul est exécuté constitue un point unique de défaillance. Cependant, cette stratégie de tolérance aux pannes nécessite de pouvoir recréer l'état du nœud en échec et cela est assez complexe) \n",
    "<br>\n",
    "<br>\n",
    "- Calculs parallèles : Dans le modèle parallèle, on passe à l'échelle de manière <font color=\"red\" > verticale</font>, en augmentant la puissance des processeurs. Mais avec le ralentissement de la loi de Moore, ce dernier modèle est remis en question.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 - Hadoop MapReduce vs Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MapReduce et Hadoop permettent d'effectuer des calculs distribués. <br>\n",
    "Cependant, à l'usage, Hadoop MapReduce présente deux inconvénients majeurs : <br>\n",
    "- Après une opération map ou reduce, le résultat doit être écrit sur disque. Ce sont ces données écrites sur disque qui permettent aux mappers et aux reducers de communiquer entre eux. C'est également l'écriture sur disque qui permet une certaine tolérance aux pannes : si une opération map ou reduce échoue, il suffit de lire les données à partir du disque pour reprendre là où on en était. Cependant, ces écritures et lectures sont coûteuses en temps. <br>\n",
    "- Le jeu d'expressions composé exclusivement d'opérations map et reduce est très limité et peu expressif. En d'autres termes, il est difficile d'exprimer des opérations complexes en n'utilisant que cet ensemble de deux opérations. <br>\n",
    "<br>\n",
    "Apache Spark est une alternative à Hadoop MapReduce pour le calcul distribué qui vise à résoudre ces deux problèmes. C'est par conséquent ce framework de calcul distribué que nous allons utiliser dans ce TP. <br>\n",
    "\n",
    "\n",
    "La différence fondamentale entre Hadoop MapReduce et Spark est que Spark écrit les données en RAM, et non sur disque. Ceci a plusieurs conséquences importantes sur la rapidité de traitement des calculs ainsi que sur l'architecture globale de Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Chargement des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as fn\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover, Word2Vec, RegexTokenizer\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, NaiveBayes\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Configuration de l'enviromment Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init('/Users/winnievorihilala/Documents/Spark/spark/spark-2.4.5-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x111f882b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://air-de-winnie:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://air-de-winnie:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x111f74cc0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkContext est le point d'entrée principal pour la fonctionnalité Spark. Un SparkContext représente la connexion à un cluster Spark et peut être utilisé pour créer des RDD, des accumulateurs et des variables de diffusion sur ce cluster.\n",
    "Un seul SparkContext peut être actif par JVM. On doit stopper sc.stop() le SparkContext actif avant d'en créer un nouveau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction getOrCreate() peut être utilisée pour obtenir ou instancier un SparkContext et l'enregistrer en tant qu'objet singleton. Étant donné que nous ne pouvons avoir qu'un seul SparkContext actif par machine virtuelle Java, cela est utile lorsque les applications peuvent souhaiter partager un SparkContext."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La connexion a un cluster Spark ici prend par défaut comme master local et comme nom d'application pyspark-shell. Ces attributs sont modifiables. Le local[*] indique que j'utilise un cluster local, qui est une autre façon de dire que je travaille en mode mono-ordinateur. Le * indique à Spark de créer autant de threads de travail que de coeurs logiques sur mon ordinateur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici ci-après un schéma pour illustrer SparkContext dans pySpark: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    <center> <strong> SparkContext </strong> </center> <br />\n",
    "    <img src=\"SparkContext.png\" alt=\"SparkContext\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Chargement des données et visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.1 - Chargement des données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = spark.read.json(\"train.json\")\n",
    "df_test = spark.read.json(\"test.json\")\n",
    "df_noclass = spark.read.json('noclass.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le dataset df_train contient 128401 lignes\n",
      "Le dataset df_test contient 76759 lignes\n",
      "Le dataset df_noclass contient 51681 lignes\n"
     ]
    }
   ],
   "source": [
    "print(\"Le dataset df_train contient\", df_train.count(),\"lignes\")\n",
    "print(\"Le dataset df_test contient\", df_test.count(),\"lignes\")\n",
    "print(\"Le dataset df_noclass contient\", df_noclass.count(),\"lignes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.2 - Visualisation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2.1 - Visualisation du jeu de données train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+\n",
      "|summary|             message|          polarity|\n",
      "+-------+--------------------+------------------+\n",
      "|  count|              128401|            128401|\n",
      "|   mean|              2336.5|2.0849058807953207|\n",
      "| stddev|   637.1032098490793| 1.998204716216392|\n",
      "|    min|! A perdu 2 jeux ...|                 0|\n",
      "|    max|ߧ  ǿ     ж  ؜    ...|                 4|\n",
      "+-------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|             message|polarity|\n",
      "+--------------------+--------+\n",
      "|! Comment était l...|       0|\n",
      "|! d'accord! Va-t-...|       0|\n",
      "|!!! Taihen desu n...|       0|\n",
      "|!!!! Auto-dj .. c...|       0|\n",
      "|!!!! Ce n'est que...|       0|\n",
      "|\"Aimant\" Le jour ...|       0|\n",
      "|\"Attrape-moi si t...|       0|\n",
      "|\"Beverley Knight\"...|       0|\n",
      "|\"Crack ... break ...|       0|\n",
      "|\"Désolé\" une cond...|       0|\n",
      "+--------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_train = df_train.stat.freqItems([\"polarity\"], 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(polarity_freqItems=['4', '0'])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_train.collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- message: string (nullable = true)\n",
      " |-- polarity: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|polarity|count|\n",
      "+--------+-----+\n",
      "|       4|66926|\n",
      "|       0|61475|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.groupBy(\"polarity\") \\\n",
    "    .count() \\\n",
    "    .orderBy(fn.col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2.2 - Visualisation du jeu de données test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+\n",
      "|summary|             message|          polarity|\n",
      "+-------+--------------------+------------------+\n",
      "|  count|               76759|             76759|\n",
      "|   mean|                null|  2.07761956252687|\n",
      "| stddev|                null|1.9985062513923468|\n",
      "|    min|! C'est tellement...|                 0|\n",
      "|    max|être un begfriend...|                 4|\n",
      "+-------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|             message|polarity|\n",
      "+--------------------+--------+\n",
      "|! Et si affamé ma...|       0|\n",
      "|! Identica présen...|       0|\n",
      "|! Je vais enfin m...|       0|\n",
      "|!?!? C'est un jou...|       0|\n",
      "|\"Easy\" qu Le plan...|       0|\n",
      "|\"Empire du soleil...|       0|\n",
      "|\"Heart\" quot; N'e...|       0|\n",
      "|\"I need\" & quot; ...|       0|\n",
      "|\"Ils ont trouvé s...|       0|\n",
      "|\"Je vérifie mon t...|       0|\n",
      "+--------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_test = df_test.stat.freqItems([\"polarity\"], 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(polarity_freqItems=['4', '0'])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_test.collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- message: string (nullable = true)\n",
      " |-- polarity: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|polarity|count|\n",
      "+--------+-----+\n",
      "|       4|39869|\n",
      "|       0|36890|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.groupBy(\"polarity\") \\\n",
    "    .count() \\\n",
    "    .orderBy(fn.col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2.3 - Visualisation du jeu de données noclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|             message|\n",
      "+-------+--------------------+\n",
      "|  count|               51681|\n",
      "|   mean|                null|\n",
      "| stddev|                null|\n",
      "|    min|! @ # $ Lundi de ...|\n",
      "|    max|œil rose? Ce n'es...|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_noclass.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             message|\n",
      "+--------------------+\n",
      "|\"Dans ganga\" Ne v...|\n",
      "|\"Dieu elton\" vous...|\n",
      "|\"I was up up the ...|\n",
      "|\"Quasi\" toute la ...|\n",
      "|# $ & Amp; * # vi...|\n",
      "|& Amp; Je parle d...|\n",
      "|& Amp; nd je ne p...|\n",
      "|& Gt; Pas bon à p...|\n",
      "|& Lt; 3 il semble...|\n",
      "|& Lt; 3 shopping ...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_noclass.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_noclass = df_noclass.stat.freqItems([\"message\"], 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(message_freqItems=[\"Pardonnez-moi à mon frère de ne pas payer la facture du câble mais puis-je surveiller le jeu des cavs dans le lit de quelqu'un? Ou des ailes\"])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_noclass.collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- message: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_noclass.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|             message|count|\n",
      "+--------------------+-----+\n",
      "|           Mercinull|   58|\n",
      "|       Nettoyez-moi!|   58|\n",
      "|    Je vous remercie|   33|\n",
      "|       Merci pour le|   20|\n",
      "|Je ne peux pas me...|   19|\n",
      "|          Merci!null|   18|\n",
      "|   Je vous remercie!|   17|\n",
      "|          bonne nuit|   16|\n",
      "|           moi aussi|   15|\n",
      "|         Bonjournull|   15|\n",
      "|Si vous aimez rir...|   14|\n",
      "|je suis perdu. Ai...|   13|\n",
      "|    Merci pour le ff|    8|\n",
      "|        Bonjour!null|    8|\n",
      "|            il pleut|    7|\n",
      "|Déclaration de mi...|    7|\n",
      "| Merci pour le suivi|    7|\n",
      "|           je t'aime|    7|\n",
      "|             de rien|    7|\n",
      "|          je connais|    7|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_noclass.groupBy(\"message\") \\\n",
    "    .count() \\\n",
    "    .orderBy(fn.col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation : pour aller plus loin : https://spark.apache.org/docs/2.1.1/api/python/pyspark.sql.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2.4 - Analyse du jeu de données "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut constater à travers à la visualisation des données que : \n",
    "- les données (train, test et no class) contiennent des ponctions, ainsi que des majuscules. Pour obtenir de meilleures performances, une normalisation des données sera par conséquent nécessaire.\n",
    "- les données de train et de test sont composées de 2 colonnes : message et polarity. La colonne message contient des chaînes de caractères et la colonne polarity contient les chiffres 0 et 4. Chaque chaîne de caractère est associée à un chiffre 0 ou 4. On peut considérer que nos jeux de données sont donc constitués d'un corpus de texte français, labellisé en 2 classes : 0 et 4.\n",
    "- les chaines de caractères composants notre corpus dans train et test sont quasiment réparties équitablement entre les 2 classes 0 et 4.\n",
    "- les données noclass sont constituées d'une unique colonne message. C'est sur ce jeu de données que nous devons effectuer notre prédiction et déterminer ainsi pour chaque chaîne de caractère de noclass, si elle appartient à la classe 0 ou à la classe 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 - Pré-traitement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(x):\n",
    "    import nltk\n",
    "    return nltk.word_tokenize(x)\n",
    "\n",
    "def pos_tag(x):\n",
    "    import nltk\n",
    "    return nltk.pos_tag([x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.sparkContext.textFile('train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = data.flatMap(word_tokenize)\n",
    "words.saveAsTextFile('train_tokens')\n",
    "\n",
    "pos_word = words.map(pos_tag)\n",
    "pos_word.saveAsTextFile('train_token_pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[224] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[225] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 - Fonction de pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langid\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/4c/0fb7d900d3b0b9c8703be316fbddffecdab23c64e1b46c7a83561d78bd43/langid-1.1.6.tar.gz (1.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.9MB 1.4MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /anaconda3/lib/python3.7/site-packages (from langid) (1.16.2)\n",
      "Building wheels for collected packages: langid\n",
      "  Building wheel for langid (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/winnievorihilala/Library/Caches/pip/wheels/29/bc/61/50a93be85d1afe9436c3dc61f38da8ad7b637a38af4824e86e\n",
      "Successfully built langid\n",
      "Installing collected packages: langid\n",
      "Successfully installed langid-1.1.6\n"
     ]
    }
   ],
   "source": [
    "!pip install langid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preproc #file : preproc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PerceptronTagger',\n",
       " 'WordNetLemmatizer',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " 'check_blanks',\n",
       " 'check_lang',\n",
       " 'langid',\n",
       " 'lemmatize',\n",
       " 're',\n",
       " 'remove_features',\n",
       " 'remove_stops',\n",
       " 'stopwords',\n",
       " 'string',\n",
       " 'tag_and_remove',\n",
       " 'tagger']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(preproc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 - 1ère méthode de pre processing : nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "import preproc as pp \n",
    "# Register all the functions in Preproc with Spark Context\n",
    "check_lang_udf = udf(pp.check_lang, StringType())\n",
    "remove_stops_udf = udf(pp.remove_stops, StringType())\n",
    "remove_features_udf = udf(pp.remove_features, StringType())\n",
    "tag_and_remove_udf = udf(pp.tag_and_remove, StringType())\n",
    "lemmatize_udf = udf(pp.lemmatize, StringType())\n",
    "check_blanks_udf = udf(pp.check_blanks, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict language and filter out those with less than 90% chance of being French\n",
    "lang_df_train = df_train.withColumn(\"lang\", check_lang_udf(df_train[\"message\"]))\n",
    "fr_df_train = lang_df_train.filter(lang_df_train[\"lang\"] == \"fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+----+\n",
      "|             message|polarity|lang|\n",
      "+--------------------+--------+----+\n",
      "|! Comment était l...|       0|  NA|\n",
      "|! d'accord! Va-t-...|       0|  NA|\n",
      "|!!! Taihen desu n...|       0|  NA|\n",
      "|!!!! Auto-dj .. c...|       0|  NA|\n",
      "|!!!! Ce n'est que...|       0|  NA|\n",
      "+--------------------+--------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lang_df_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+----+\n",
      "|             message|polarity|lang|\n",
      "+--------------------+--------+----+\n",
      "|        Cours nooooo|       0|  fr|\n",
      "|Horrible n'est-ce...|       0|  fr|\n",
      "|           Le hoquet|       0|  fr|\n",
      "|          Maths suce|       0|  fr|\n",
      "|             Mec fyl|       0|  fr|\n",
      "+--------------------+--------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fr_df_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words to reduce dimensionality\n",
    "rm_stops_df_train = fr_df_train.withColumn(\"stop_message\", remove_stops_udf(fr_df_train[\"message\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+----+--------------------+\n",
      "|             message|polarity|lang|        stop_message|\n",
      "+--------------------+--------+----+--------------------+\n",
      "|        Cours nooooo|       0|  fr|        Cours nooooo|\n",
      "|Horrible n'est-ce...|       0|  fr|Horrible n'est-ce...|\n",
      "|           Le hoquet|       0|  fr|           Le hoquet|\n",
      "|          Maths suce|       0|  fr|          Maths suce|\n",
      "|             Mec fyl|       0|  fr|             Mec fyl|\n",
      "+--------------------+--------+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rm_stops_df_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove other non essential words, think of it as my personal stop word list\n",
    "rm_features_df_train = rm_stops_df_train.withColumn(\"feat_message\", remove_features_udf(rm_stops_df_train[\"stop_message\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+----+--------------------+------------------+\n",
      "|             message|polarity|lang|        stop_message|      feat_message|\n",
      "+--------------------+--------+----+--------------------+------------------+\n",
      "|        Cours nooooo|       0|  fr|        Cours nooooo|      cours nooooo|\n",
      "|Horrible n'est-ce...|       0|  fr|Horrible n'est-ce...|horrible  est  pas|\n",
      "|           Le hoquet|       0|  fr|           Le hoquet|            hoquet|\n",
      "|          Maths suce|       0|  fr|          Maths suce|        maths suce|\n",
      "|             Mec fyl|       0|  fr|             Mec fyl|           mec fyl|\n",
      "+--------------------+--------+----+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rm_features_df_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- message: string (nullable = true)\n",
      " |-- polarity: string (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- stop_message: string (nullable = true)\n",
      " |-- feat_message: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rm_features_df_train.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag the words remaining and keep only Nouns, Verbs and Adjectives\n",
    "tagged_df_train = rm_features_df_train.withColumn(\"tagged_message\", tag_and_remove_udf(rm_features_df_train[\"feat_message\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+----+--------------------+------------------+------------------+\n",
      "|             message|polarity|lang|        stop_message|      feat_message|    tagged_message|\n",
      "+--------------------+--------+----+--------------------+------------------+------------------+\n",
      "|        Cours nooooo|       0|  fr|        Cours nooooo|      cours nooooo|     cours nooooo |\n",
      "|Horrible n'est-ce...|       0|  fr|Horrible n'est-ce...|horrible  est  pas| horrible est pas |\n",
      "|           Le hoquet|       0|  fr|           Le hoquet|            hoquet|           hoquet |\n",
      "|          Maths suce|       0|  fr|          Maths suce|        maths suce|       maths suce |\n",
      "|             Mec fyl|       0|  fr|             Mec fyl|           mec fyl|          mec fyl |\n",
      "+--------------------+--------+----+--------------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tagged_df_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- message: string (nullable = true)\n",
      " |-- polarity: string (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- stop_message: string (nullable = true)\n",
      " |-- feat_message: string (nullable = true)\n",
      " |-- tagged_message: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tagged_df_train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization of remaining words to reduce dimensionality & boost measures\n",
    "lemm_df_train = tagged_df_train.withColumn(\"lemm_message\", lemmatize_udf(tagged_df_train[\"tagged_message\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+----+--------------------+------------------+------------------+---------------+\n",
      "|             message|polarity|lang|        stop_message|      feat_message|    tagged_message|   lemm_message|\n",
      "+--------------------+--------+----+--------------------+------------------+------------------+---------------+\n",
      "|        Cours nooooo|       0|  fr|        Cours nooooo|      cours nooooo|     cours nooooo |   cours nooooo|\n",
      "|Horrible n'est-ce...|       0|  fr|Horrible n'est-ce...|horrible  est  pas| horrible est pas |horrible est pa|\n",
      "|           Le hoquet|       0|  fr|           Le hoquet|            hoquet|           hoquet |         hoquet|\n",
      "|          Maths suce|       0|  fr|          Maths suce|        maths suce|       maths suce |      math suce|\n",
      "|             Mec fyl|       0|  fr|             Mec fyl|           mec fyl|          mec fyl |        mec fyl|\n",
      "+--------------------+--------+----+--------------------+------------------+------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lemm_df_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- message: string (nullable = true)\n",
      " |-- polarity: string (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- stop_message: string (nullable = true)\n",
      " |-- feat_message: string (nullable = true)\n",
      " |-- tagged_message: string (nullable = true)\n",
      " |-- lemm_message: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lemm_df_train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all rows containing only blank spaces\n",
    "check_blanks_df_train = lemm_df_train.withColumn(\"is_blank\", check_blanks_udf(lemm_df_train[\"lemm_message\"]))\n",
    "no_blanks_df_train = check_blanks_df_train.filter(check_blanks_df_train[\"is_blank\"] == \"False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+----+--------------------+------------------+------------------+---------------+--------+\n",
      "|             message|polarity|lang|        stop_message|      feat_message|    tagged_message|   lemm_message|is_blank|\n",
      "+--------------------+--------+----+--------------------+------------------+------------------+---------------+--------+\n",
      "|        Cours nooooo|       0|  fr|        Cours nooooo|      cours nooooo|     cours nooooo |   cours nooooo|   False|\n",
      "|Horrible n'est-ce...|       0|  fr|Horrible n'est-ce...|horrible  est  pas| horrible est pas |horrible est pa|   False|\n",
      "|           Le hoquet|       0|  fr|           Le hoquet|            hoquet|           hoquet |         hoquet|   False|\n",
      "|          Maths suce|       0|  fr|          Maths suce|        maths suce|       maths suce |      math suce|   False|\n",
      "|             Mec fyl|       0|  fr|             Mec fyl|           mec fyl|          mec fyl |        mec fyl|   False|\n",
      "+--------------------+--------+----+--------------------+------------------+------------------+---------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_blanks_df_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- message: string (nullable = true)\n",
      " |-- polarity: string (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- stop_message: string (nullable = true)\n",
      " |-- feat_message: string (nullable = true)\n",
      " |-- tagged_message: string (nullable = true)\n",
      " |-- lemm_message: string (nullable = true)\n",
      " |-- is_blank: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_blanks_df_train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+----+--------------------+------------------+------------------+---------------+--------+\n",
      "|             message|polarity|lang|        stop_message|      feat_message|    tagged_message|   lemm_message|is_blank|\n",
      "+--------------------+--------+----+--------------------+------------------+------------------+---------------+--------+\n",
      "|        Cours nooooo|       0|  fr|        Cours nooooo|      cours nooooo|     cours nooooo |   cours nooooo|   False|\n",
      "|Horrible n'est-ce...|       0|  fr|Horrible n'est-ce...|horrible  est  pas| horrible est pas |horrible est pa|   False|\n",
      "|           Le hoquet|       0|  fr|           Le hoquet|            hoquet|           hoquet |         hoquet|   False|\n",
      "|          Maths suce|       0|  fr|          Maths suce|        maths suce|       maths suce |      math suce|   False|\n",
      "|             Mec fyl|       0|  fr|             Mec fyl|           mec fyl|          mec fyl |        mec fyl|   False|\n",
      "+--------------------+--------+----+--------------------+------------------+------------------+---------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "no_blanks_df_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- message: string (nullable = true)\n",
      " |-- polarity: string (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- stop_message: string (nullable = true)\n",
      " |-- feat_message: string (nullable = true)\n",
      " |-- tagged_message: string (nullable = true)\n",
      " |-- lemm_message: string (nullable = true)\n",
      " |-- is_blank: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "no_blanks_df_train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only the columns we care about\n",
    "train_data_set = no_blanks_df_train.select(no_blanks_df_train['lemm_message'], no_blanks_df_train['polarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- lemm_message: string (nullable = true)\n",
      " |-- polarity: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data_set.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_set = train_data_set.withColumnRenamed(\"lemm_message\", \"message\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- message: string (nullable = true)\n",
      " |-- polarity: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data_set.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|             message|polarity|\n",
      "+--------------------+--------+\n",
      "|        cours nooooo|       0|\n",
      "|     horrible est pa|       0|\n",
      "|              hoquet|       0|\n",
      "|           math suce|       0|\n",
      "|             mec fyl|       0|\n",
      "|                  pa|       0|\n",
      "|     toooooday pleut|       0|\n",
      "|        urgent lundi|       0|\n",
      "|         fonctionner|       0|\n",
      "|veut son argent rudd|       0|\n",
      "|              bureau|       0|\n",
      "|             le sims|       0|\n",
      "|              pa bon|       0|\n",
      "|              pa bon|       0|\n",
      "|            toutnull|       0|\n",
      "|     vraiment triste|       0|\n",
      "|            matthieu|       0|\n",
      "|                 plu|       0|\n",
      "|              raison|       0|\n",
      "|   suppose est parti|       0|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data_set.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_set.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|polarity|count|\n",
      "+--------+-----+\n",
      "|       4|   73|\n",
      "|       0|   46|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data_set.groupBy(\"polarity\") \\\n",
    "    .count() \\\n",
    "    .orderBy(fn.col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------------+\n",
      "|summary|    message|          polarity|\n",
      "+-------+-----------+------------------+\n",
      "|  count|        119|               119|\n",
      "|   mean|       null| 2.453781512605042|\n",
      "| stddev|       null|1.9560765779941627|\n",
      "|    min|           |                 0|\n",
      "|    max|yeay accord|                 4|\n",
      "+-------+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data_set.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|             message|polarity|\n",
      "+--------------------+--------+\n",
      "|        cours nooooo|       0|\n",
      "|     horrible est pa|       0|\n",
      "|              hoquet|       0|\n",
      "|           math suce|       0|\n",
      "|             mec fyl|       0|\n",
      "|                  pa|       0|\n",
      "|     toooooday pleut|       0|\n",
      "|        urgent lundi|       0|\n",
      "|         fonctionner|       0|\n",
      "|veut son argent rudd|       0|\n",
      "|              bureau|       0|\n",
      "|             le sims|       0|\n",
      "|              pa bon|       0|\n",
      "|              pa bon|       0|\n",
      "|            toutnull|       0|\n",
      "|     vraiment triste|       0|\n",
      "|            matthieu|       0|\n",
      "|                 plu|       0|\n",
      "|              raison|       0|\n",
      "|   suppose est parti|       0|\n",
      "|              hoquet|       0|\n",
      "|            med suck|       0|\n",
      "|      mon ventre tue|       0|\n",
      "|         est dommage|       0|\n",
      "|         est dommage|       0|\n",
      "|                 bbm|       0|\n",
      "|           peut suce|       0|\n",
      "|              est pa|       0|\n",
      "|         partoutnull|       0|\n",
      "|              pa bon|       0|\n",
      "+--------------------+--------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data_set.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- message: string (nullable = true)\n",
      " |-- polarity: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#conversion du type de la colonne polarity en int\n",
    "#train_data_set = train_data_set.withColumn(\"polarity\", train_data_set[\"polarity\"].cast('int'))\n",
    "df_train = df_train.withColumn(\"polarity\", df_train[\"polarity\"].cast('int'))\n",
    "df_test = df_test.withColumn(\"polarity\", df_test[\"polarity\"].cast('int'))\n",
    "df_train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training & validation sets with 80% to training and use a seed value of 1987\n",
    "splits = train_data_set.randomSplit([0.8, 0.2])\n",
    "training_df = splits[0]\n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec cette première méthode de pre processing, on constate que les données sont bien normalisées ( plus de ponctuations, ni de stopwords français (préalablement définis dans la librairie ntlk et composés d'un ensemble de mots courants tels que de, je, ...),  plus de caractères spéciaux et les majuscules ont été transformés en minuscule). Cependant, on passe d'un jeu de donnée non pré-traité faisant 128401 lignes à un jeu de donnée pré-traité faisant 119 lignes, soit une perte d'informations de 99% ce qui n'est absolument pas normal. Par conséquent, les mots stopwords à soustraire à un jeu de données dépendent de notre jeu de données. Dans notre cas, le données ne présentant pas une quantité de bruit énorme, il est préférable de spécifier à la main les stopwords à enlever."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 - 2ème méthode de pre processing : regexTokenizer et stopwordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover, RegexTokenizer\n",
    "\n",
    "add_stopwords = [\"!\",\"!!!\",\"...\",\"..\",\"-\",\"*\",\".\",\"!\",\"?\",\"Gt\",\"Lt\",\"gt\",\"lt\",\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\",\"null\",\"&\",\"Quot\", \"quot\"] \n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"message\", outputCol=\"words\", pattern=\"\\\\W\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 - Extraction de features avec CountVector, TF IDF, word2vec et StringIndexer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import HashingTF, IDF, StringIndexer\n",
    "\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=4)\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=4)\n",
    "w2v = Word2Vec(inputCol= 'filtered', outputCol= 'features', vectorSize= 100)\n",
    "label_stringIdx = StringIndexer(inputCol = \"polarity\", outputCol = \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 - Classification de sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.1 -  Clasification avec NaivesBayes et Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes, LogisticRegression\n",
    "\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.2 - Prediction sur les données de test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.2.1 - 1ère méthode : avec CountVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Le nom du modèle utilisé est : naive_bayes\n",
      "Le score obtenu sur les données d'entraînement est : 0.790\n",
      "Le Score sur les données de test est: 0.768 \n",
      "\n",
      "\n",
      "Le nom du modèle utilisé est : logistic_regression\n",
      "Le score obtenu sur les données d'entraînement est : 0.802\n",
      "Le Score sur les données de test est: 0.769 \n",
      "\n",
      "CPU times: user 266 ms, sys: 66.4 ms, total: 333 ms\n",
      "Wall time: 27.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "predictions = {}\n",
    "models = [nb, lr]\n",
    "name_models = ['naive_bayes','logistic_regression']\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx, model])\n",
    "    mod = pipeline.fit(df_train)\n",
    "    prediction_train = mod.transform(df_train)\n",
    "    prediction_test = mod.transform(df_test)\n",
    "    \n",
    "    predictions['prediction_train_%s_%s'%(name_models[i],'CountVectorizer')] = prediction_train\n",
    "    predictions['prediction_test_%s_%s'%(name_models[i],'CountVectorizer')] = prediction_test\n",
    "    \n",
    "    print(\"\\nLe nom du modèle utilisé est : %s\" %(name_models[i]))\n",
    "    print(\"Le score obtenu sur les données d'entraînement est : %.3f\" %(evaluator.evaluate(prediction_train)))\n",
    "    print(\"Le Score sur les données de test est: %.3f \\n\" %(evaluator.evaluate(prediction_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.2.2 - 2ème méthode : avec TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Le nom du modèle utilisé est : naive_bayes\n",
      "Le score obtenu sur les données d'entraînement est : 0.773\n",
      "Le Score sur les données de test est: 0.736 \n",
      "\n",
      "\n",
      "Le nom du modèle utilisé est : logistic_regression\n",
      "Le score obtenu sur les données d'entraînement est : 0.786\n",
      "Le Score sur les données de test est: 0.751 \n",
      "\n",
      "CPU times: user 320 ms, sys: 79.3 ms, total: 399 ms\n",
      "Wall time: 33.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "models = [nb, lr]\n",
    "name_models = ['naive_bayes','logistic_regression']\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, hashingTF, idf, label_stringIdx, model])\n",
    "    mod = pipeline.fit(df_train)\n",
    "    prediction_train = mod.transform(df_train)\n",
    "    prediction_test = mod.transform(df_test)\n",
    "    \n",
    "    predictions['prediction_train_%s_%s'%(name_models[i],'tf_idf')] = prediction_train\n",
    "    predictions['prediction_test_%s_%s'%(name_models[i],'tf_idf')] = prediction_test\n",
    "    \n",
    "    print(\"\\nLe nom du modèle utilisé est : %s\" %(name_models[i]))\n",
    "    print(\"Le score obtenu sur les données d'entraînement est : %.3f\" %(evaluator.evaluate(prediction_train)))\n",
    "    print(\"Le Score sur les données de test est: %.3f \\n\" %(evaluator.evaluate(prediction_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour un temps d'execution pratiquement identique, nous obtenons de meilleures performances avec CountVectorizer plutôt qu'avec TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+-----+----------+\n",
      "|                       message|label|prediction|\n",
      "+------------------------------+-----+----------+\n",
      "|! Et si affamé mais pas de ...|  1.0|       1.0|\n",
      "|! Identica présente actuell...|  1.0|       0.0|\n",
      "|! Je vais enfin m'endormir ...|  1.0|       1.0|\n",
      "|!?!? C'est un jour que j'ai...|  1.0|       1.0|\n",
      "|\"Easy\" qu Le plancher en bo...|  1.0|       0.0|\n",
      "|\"Empire du soleil\" L'auteur...|  1.0|       1.0|\n",
      "|\"Heart\" quot; N'est pas un ...|  1.0|       1.0|\n",
      "|\"I need\" & quot; -? V1-1333...|  1.0|       1.0|\n",
      "|\"Ils ont trouvé sonny? & Qu...|  1.0|       0.0|\n",
      "|\"Je vérifie mon twitter cha...|  1.0|       1.0|\n",
      "|\"Recevoir un préavis avec t...|  1.0|       0.0|\n",
      "|\"Statut sélectif de twitter...|  1.0|       1.0|\n",
      "|\"Sur la musique populaire &...|  1.0|       1.0|\n",
      "|\"The dating expert\" Me suit...|  1.0|       0.0|\n",
      "|& Amp; Tout le reste je n'a...|  1.0|       1.0|\n",
      "|& Gt; Le frère est sur son ...|  1.0|       1.0|\n",
      "|& Gt; Que cela impliquera l...|  1.0|       0.0|\n",
      "|& Gt; __ & lt; Je suis prob...|  1.0|       0.0|\n",
      "|& Lt; & lt; ---------------...|  1.0|       1.0|\n",
      "|& Lt; --- ne peut pas chant...|  1.0|       0.0|\n",
      "+------------------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions['prediction_test_logistic_regression_CountVectorizer'] \\\n",
    "    .select(\"message\",\"label\",\"prediction\") \\\n",
    "    .show(n = 20, truncate = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.3 -  Cross validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pyspark offre la possibilité d'utilsier des fonctions telles que CrossValidator et ParamGridBuilder pour effectuer la cross validation et optimiser ainsi les performances des modèles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nom du modèle utilisé : logistic regression\n",
      "Score sur les données d'entraînement : 0.809\n",
      "Score sur les données test : 0.778 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#temps d'execution, environ 45 min\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "lr = LogisticRegression()\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx, lr])\n",
    "vectC = np.logspace(-3, -2, 20)\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, vectC) \n",
    "             .addGrid(lr.elasticNetParam, [0.1, 0.15]) \n",
    "             .build())\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "\n",
    "cv = CrossValidator(estimator=pipeline, \\\n",
    "                    estimatorParamMaps=paramGrid, \\\n",
    "                    evaluator=evaluator, \\\n",
    "                    numFolds=10)\n",
    "cvModel_lr = cv.fit(df_train)\n",
    "\n",
    "predictions_test = cvModel_lr.transform(df_test)\n",
    "predictions_train = cvModel_lr.transform(df_train)\n",
    "print(\"\\nNom du modèle utilisé : logistic regression\")\n",
    "print(\"Score sur les données d'entraînement : %.3f\" %(evaluator.evaluate(predictions_train)))\n",
    "print(\"Score sur les données test : %.3f \\n\" %(evaluator.evaluate(predictions_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Le nom du modèle utilisé est : naive bayes\n",
      "Le score obtenu sur les données d'entraînement est : 0.789\n",
      "Le score obtenu sur les données de test est : 0.768\n",
      "CPU times: user 8.08 s, sys: 2.04 s, total: 10.1 s\n",
      "Wall time: 3min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "nb = NaiveBayes(modelType=\"multinomial\")\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx, nb])\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(nb.smoothing, [0,1,2,5,10]) \n",
    "             .build())\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "\n",
    "cv = CrossValidator(estimator=pipeline, \\\n",
    "                    estimatorParamMaps=paramGrid, \\\n",
    "                    evaluator=evaluator, \\\n",
    "                    numFolds=10)\n",
    "cvModel_nb = cv.fit(df_train)\n",
    "\n",
    "predictions_test = cvModel_nb.transform(df_test)\n",
    "predictions_train = cvModel_nb.transform(df_train)\n",
    "\n",
    "print(\"\\nLe nom du modèle utilisé est : naive bayes\")\n",
    "print(\"Le score obtenu sur les données d'entraînement est : %.3f\" %(evaluator.evaluate(predictions_train)))\n",
    "print(\"Le score obtenu sur les données de test est : %.3f\" %(evaluator.evaluate(predictions_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La meilleure valeur à attribuer au paramètre regParam de Logistic Regression est : 0.009\n",
      "\n",
      "Best elastic_net à attribuer au paramètre regParam de Logistic Regression est : 0.100\n",
      " \n",
      "CPU times: user 473 µs, sys: 256 µs, total: 729 µs\n",
      "Wall time: 532 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "bestModel = cvModel_lr.bestModel\n",
    "param_dict = bestModel.stages[-1].extractParamMap()\n",
    "\n",
    "sane_dict = {}\n",
    "for k, v in param_dict.items():\n",
    "    sane_dict[k.name] = v\n",
    "\n",
    "print('La meilleure valeur à attribuer au paramètre regParam de Logistic Regression est : %.3f' %sane_dict[\"regParam\"])\n",
    "print('\\nBest elastic_net à attribuer au paramètre regParam de Logistic Regression est : %.3f' %sane_dict[\"elasticNetParam\"])\n",
    "\n",
    "print( \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 - Prediction sur le jeu de données noclass.json et sauveragarde du fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|             message|prediction|\n",
      "+--------------------+----------+\n",
      "|! Et si affamé ma...|       1.0|\n",
      "|! Identica présen...|       0.0|\n",
      "|! Je vais enfin m...|       1.0|\n",
      "|!?!? C'est un jou...|       1.0|\n",
      "|\"Easy\" qu Le plan...|       0.0|\n",
      "|\"Empire du soleil...|       1.0|\n",
      "|\"Heart\" quot; N'e...|       1.0|\n",
      "|\"I need\" & quot; ...|       1.0|\n",
      "|\"Ils ont trouvé s...|       0.0|\n",
      "|\"Je vérifie mon t...|       0.0|\n",
      "|\"Recevoir un préa...|       0.0|\n",
      "|\"Statut sélectif ...|       1.0|\n",
      "|\"Sur la musique p...|       1.0|\n",
      "|\"The dating exper...|       0.0|\n",
      "|& Amp; Tout le re...|       1.0|\n",
      "|& Gt; Le frère es...|       1.0|\n",
      "|& Gt; Que cela im...|       0.0|\n",
      "|& Gt; __ & lt; Je...|       0.0|\n",
      "|& Lt; & lt; -----...|       1.0|\n",
      "|& Lt; --- ne peut...|       0.0|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_noclass = cvModel_lr.transform(df_test).select(\"message\",\"prediction\")\n",
    "predictions_noclass = predictions_noclass.withColumn(\"prediction\", predictions_noclass[\"prediction\"].cast('string'))\n",
    "predictions_noclass.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sauvegarde du fichier :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_noclass.coalesce(1).write.format('json').save('noclass_predit.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - RECAPITULATIF DES METHODES UTILISEES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour pré-processer nos données, nous avons utilisé 2 méthodes : <br>\n",
    "\n",
    "- <p style=\"text-align:justify;\"><strong> ntlk : </strong> NLTK signifie Natural Language Toolkit. Cette boîte à outils est l'une des bibliothèques NLP les plus puissantes qui contient des packages permettant aux machines de comprendre le langage humain et d'y répondre avec une réponse appropriée. Tokenization, Stemming, Lemmatization, Punctuation, Character count, word count sont quelques-uns de ces packages.<br> </p>\n",
    "<br>\n",
    "- <p style=\"text-align:justify;\"><strong> regexTokenizer et stopWordsRemover </strong> : La tokenisation est le processus consistant à prendre du texte (comme une phrase) et à le diviser en termes individuels (généralement des mots). Une simple classe Tokenizer fournit cette fonctionnalité. L'exemple ci-dessous montre comment diviser des phrases en séquences de mots. <br>\n",
    "RegexTokenizer permet une tokenisation plus avancée basée sur la correspondance d'expressions régulières (regex). Par défaut, le paramètre «pattern» (regex, default:) \"\\\\s+\"est utilisé comme délimiteur pour diviser le texte d'entrée. Alternativement, les utilisateurs peuvent définir le paramètre «lacunes» sur faux indiquant que le «modèle» regex désigne les «jetons» plutôt que de fractionner les lacunes, et trouver toutes les occurrences correspondantes comme résultat de la tokenisation. <br> </p>\n",
    "<br>\n",
    "\n",
    "Pour l'extraction de features, nous avons utilisé : <br>\n",
    "- <p style=\"text-align:justify;\"><strong> CountVectors : </strong> CountVectorizer et CountVectorizerModel visent à aider à convertir une collection de documents texte en vecteurs de décompte de jetons. Lorsqu'un dictionnaire a priori n'est pas disponible, CountVectorizerpeut être utilisé comme un Estimator pour extraire le vocabulaire, et génère un CountVectorizerModel. Le modèle produit des représentations clairsemées pour les documents sur le vocabulaire, qui peuvent ensuite être transmises à d'autres algorithmes comme LDA.\n",
    "<br>\n",
    "Au cours du processus d'ajustement, CountVectorizersélectionnera les meilleurs vocabSizemots classés par fréquence de terme dans le corpus. Un paramètre facultatif minDFaffecte également le processus d'ajustement en spécifiant le nombre minimum (ou la fraction si inférieur à 1,0) de documents dans lesquels un terme doit apparaître pour être inclus dans le vocabulaire. Un autre paramètre de basculement binaire facultatif contrôle le vecteur de sortie. Si la valeur est vraie, tous les nombres non nuls sont définis sur 1. Ceci est particulièrement utile pour les modèles probabilistes discrets qui modélisent les nombres binaires, plutôt qu'entiers. <br> </p>\n",
    "<br>\n",
    "- <p style=\"text-align:justify;\"> <strong> TDF IDF </strong> : TF-IDF (Term Frequency-inverse Document Frequency) est une méthode de vectorisation de features largement utilisée dans l'exploration de texte ou text mining pour refléter l'importance d'un terme pour un document dans le corpus. Notons un terme par t, un document par d et le corpus par D. <br>\n",
    "La fréquence du terme TF(t,d) est le nombre de fois où ce terme t apparaît dans le document d, tandis que la fréquence du document DF(t,D) est le nombre de documents qui contient le terme t. Si nous utilisons uniquement la fréquence des termes pour mesurer l'importance, il est très facile de surestimer les termes qui apparaissent très souvent mais qui contiennent peu d'informations sur le document, par exemple «a», «le» et «de». Si un terme apparaît très souvent dans le corpus, cela signifie qu'il ne contient pas d'informations spéciales sur un document particulier. La fréquence inverse des documents est une mesure numérique de la quantité d'informations fournies par un terme : <br> </p>\n",
    "<p>\n",
    "    <img src=\"idf.png\" alt=\"Formule de calcul de l''IDF\" />\n",
    "</p>\n",
    "<br>\n",
    "<p style=\"text-align:justify;\">où |D| est le nombre total de documents dans le corpus. Puisque le logarithme est utilisé, si un terme apparaît dans tous les documents, sa valeur IDF devient 0. Notons qu'un terme de lissage est appliqué pour éviter de diviser par zéro pour les termes en dehors du corpus. La mesure TF-IDF est simplement le produit de TF et IDF: <br> </p>\n",
    "<p>\n",
    "    <img src=\"tfidf.png\" alt=\"Formule de calcul du TFIDF\" />\n",
    "</p>\n",
    "<br>\n",
    "<p style=\"text-align:justify;\">Il existe plusieurs variantes sur la définition de la fréquence des termes et de la fréquence des documents. Dans MLlib, nous séparons TF et IDF pour les rendre flexibles. <br> </p>\n",
    "<br>\n",
    "<p style=\"text-align:justify;\"><strong> TF : </strong> Les deux HashingTFet CountVectorizerpeuvent être utilisés pour générer le terme vecteurs de fréquence. HashingTF est un transformateur qui prend des ensembles de termes et convertit ces ensembles en vecteurs d'entités de longueur fixe. Dans le traitement de texte, un «ensemble de termes» peut être un sac de mots. HashingTF utilise l'astuce de hachage. Une fonction brute est mappée dans un index (terme) en appliquant une fonction de hachage. La fonction de hachage utilisée ici est MurmurHash 3. Ensuite, les fréquences des termes sont calculées sur la base des indices cartographiés. Cette approche évite d'avoir à calculer une carte terme-index globale, ce qui peut être coûteux pour un grand corpus, mais elle souffre de collisions de hachage potentielles, où différentes caractéristiques brutes peuvent devenir le même terme après le hachage. Pour réduire les risques de collision, nous pouvons augmenter la dimension de l'entité cible, c'est-à-dire le nombre de compartiments de la table de hachage. Puisqu'un simple modulo est utilisé pour transformer la fonction de hachage en un index de colonne, il est conseillé d'utiliser une puissance de deux comme dimension d'entité, sinon les entités ne seront pas mappées uniformément aux colonnes. La dimension d'entité par défaut est : 2^18 = 262 144. Un paramètre à bascule binaire facultatif contrôle le nombre de fréquences des termes. Lorsqu'il est défini sur true, tous les nombres de fréquences non nuls sont définis sur 1. Ceci est particulièrement utile pour les modèles probabilistes discrets qui modélisent les nombres binaires plutôt qu'entiers. <br> </p>\n",
    "<br>\n",
    "<p style=\"text-align:justify;\"><strong> IDF : </strong> IDFest un Estimator qui est adapté à un ensemble de données et produit un IDFModel. La IDFModel prend des vecteurs de caractéristiques (généralement créés à partir de HashingTFou CountVectorizer) et met à l'échelle chaque colonne. Intuitivement, il pondère les colonnes qui apparaissent fréquemment dans un corpus. <br> </p>\n",
    "<br>\n",
    "\n",
    "<p style=\"text-align:justify;\">=>Voici ce qui se fait dans la pratique : supposons que nous disposons d'un ensemble de phrases. Nous allons diviser chaque phrase en mots en utilisant Tokenizer. Pour chaque phrase (sac de mots), nous utilisons HashingTF pour hacher la phrase en un vecteur de feature. Puis nous utilisons IDF pour redimensionner les vecteurs de feature; cela améliore généralement les performances lors de l'utilisation de texte comme features. Nos vecteurs de caractéristiques pourraient ensuite être passés à un algorithme d'apprentissage. </p>\n",
    "<br>\n",
    "\n",
    "- <p style=\"text-align:justify;\"><strong> Word2Vec : </strong> Word2Vec est un Estimator qui prend des séquences de mots représentant des documents et forme un Word2VecModel. Le modèle associe chaque mot à un vecteur unique de taille fixe. Le Word2VecModel transforme chaque document en vecteur en utilisant la moyenne de tous les mots du document; ce vecteur peut ensuite être utilisé comme fonctionnalités pour la prédiction. <br> </p>\n",
    "- <p style=\"text-align:justify;\"><strong> StringIndexer : </strong> StringIndexer code une colonne de chaînes d'étiquettes en une colonne d'index d'étiquettes. Les indices sont entrés (0, numLabels), classés par fréquences d'étiquettes, de sorte que l'étiquette la plus fréquente est indexée 0. Si la colonne d'entrée est numérique, nous la convertissons en chaîne et indexons les valeurs de chaîne. <br>\n",
    "Supposons que nous ayons le DataFrame suivant avec des colonnes id et category: <br> </p>\n",
    "<p>\n",
    "    <img src=\"exemple1.png\" alt=\"StringIndexer_exemple1\" />\n",
    "</p>\n",
    "<br>\n",
    "<p style=\"text-align:justify;\">category est une colonne de chaîne de caractères avec trois étiquettes: «a», «b» et «c». En appliquant StringIndexer avec category comme colonne d'entrée et categoryIndex comme colonne de sortie, nous devrions obtenir ce qui suit: </p>\n",
    "<p>\n",
    "    <img src=\"exemple2.png\" alt=\"StringIndexer_exemple2\" />\n",
    "</p>\n",
    "<br>\n",
    "«a» obtient l'index 0 car c'est le plus fréquent, suivi de «c» avec index 1 et de «b» avec index 2.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "Pour la classification de sentiments, nous avons utilisé : <br>\n",
    "<br>\n",
    "- <p style=\"text-align:justify;\"><strong> Naives Bayes :</strong> Naive Bayes est un algorithme de classification multiclasse simple avec l'hypothèse d'indépendance entre chaque paire de fonctionnalités. Naive Bayes peut être formé très efficacement. En un seul passage aux données d'apprentissage, il calcule la distribution de probabilité conditionnelle de chaque entité donnée étiquettée, puis il applique le théorème de Bayes pour calculer la distribution de probabilité conditionnelle de l'étiquette à partir d'une observation, et il va l'utiliser pour la prédiction. <br>\n",
    "spark.mllib prend en charge les Bayes naïfs multinomiaux et les Bayes naïfs de Bernoulli . Ces modèles sont généralement utilisés pour la classification des documents . Dans ce contexte, chaque observation est un document et chaque entité représente un terme dont la valeur est la fréquence du terme (dans les Bayes naïfs multinomiaux) ou un zéro ou un indiquant si le terme a été trouvé dans le document (dans les Bayes naïfs de Bernoulli). Les valeurs des entités doivent être <font color=\"red\">non négatives</font>. Le type de modèle est sélectionné avec un paramètre facultatif «multinomial» ou «bernoulli» avec «multinomial» par défaut. Le lissage additif peut être utilisé en définissant le paramètre λ (par défaut à 1.0). Pour la classification des documents, les vecteurs d'entités en entrée sont généralement clairsemés et les vecteurs clairsemés doivent être fournis en entrée pour tirer parti de la rareté. Étant donné que les données d'apprentissage ne sont utilisées qu'une seule fois, il n'est pas nécessaire de les mettre en cache. <br></p>\n",
    "<br>\n",
    "- <p style=\"text-align:justify;\"><strong> Regression logistique :</strong> la régression logistique est une méthode populaire pour prédire une réponse catégorique. Il s'agit d'un cas particulier des modèles linéaires généralisés qui prédit la probabilité des résultats. Dans spark.ml, on peut utiliser la régression logistique pour prédire un résultat binaire en utilisant la régression logistique binomiale. On peut également utiliser la régression logistique pour prédire un résultat multiclasse en utilisant la régression logistique multinomiale. Pour cela, il faut utiliser le paramètre family pour selectionner entre ces deux algorithmes. On peut également laisser ce paramètre non-défini, et dans ce cas, c'est Spark qui déduira la bonne variante. </p>\n",
    "<br>\n",
    "<br>\n",
    "<p style=\"text-align:justify;\"> Spark ML fournit d'autres méthodes de classification telles que le decision tree classifier, le random forest classifier, le gradient-boosted tree classifier, le multilayer perceptron classifier, le One-vs-Rest classifier (a.k.a.One-vs-All). Dans ce TP, nous n'avons utilisé que Naive Bayes et Logistic Regression, mais il peut être intéressant d'utiliser toutes ces méthodes et de déterminer laquelle donne la meilleure performance su notre jeu de donnée. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - DISTRIBUTION DES CALCULS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Map Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<strong> QUOI : </strong>  modèle de programmation qui fournit un cadre pour automatiser le calcul parallèle sur des données massives. <br>\n",
    "<br>\n",
    "MapReduce a été conçu chez Google car c'est une opération nécessaire au calcul du fameux PageRank, utilisé pour ordonnancer les résultats d'une recherche Web. <br>\n",
    "Ce modèle a été proposé dans les années 2000, par deux ingénieurs de chez Google, qui ont observé qu'un grand nombre des traitements massivement parallèles, mis en place pour les besoins de leur moteur de recherche, suivaient une stratégie de parallélisation identique. De ces observations est né le modèle de programmation MapReduce, décrit pour la première fois en 2004 dans un article de recherche. (link : https://static.googleusercontent.com/media/research.google.com/fr//archive/mapreduce-osdi04.pdf) \n",
    "<br>\n",
    "\n",
    "<p style=\"text-align:justify;\"><strong> COMMENT : </strong> MapReduce c'est \"divisez pour distribuer pour régner\" en ce sens que la stratégie mise en place pour exécuter un calcul sur des données massives consiste à découper les données en sous-ensembles de plus petite taille, qui sont appelés des lots ou des fragments, et à affecter chaque lot à une machine du cluster permettant ainsi leur traitement en parallèle. Il suffira ensuite d'agréger l'ensemble des résultats intermédiaires obtenus pour chaque lot pour construire le résultat final. <br>\n",
    "* MAP consiste à appliquer une même fonction à tous les éléments de la liste; <br>\n",
    "* REDUCE applique une fonction récursivement à une liste et retourne un seul résultat; <br>\n",
    "Map et Reduce sont des opérateurs génériques et leur combinaison permet donc de modéliser énormément de problèmes. <br> </p>\n",
    "L'ensemble des données est représentée sous la forme de paires(clé, valeur), à l'instar des tables d'association. <br>\n",
    "\n",
    "Pour résumer :  <br>\n",
    "\n",
    "1-L'ensemble des données à traiter est découpé en plusieurs lots ou sous-ensembles.<br>\n",
    "\n",
    "2-Dans une première étape, l'étape MAP, l'opération map, spécifiée pour notre problème, est appliquée à chaque lot. Cette opération transforme la paire(clé, valeur)représentant le lot en une liste de nouvelles paires(clé, valeur)constituant ainsi des résultats intermédiaires du traitement à effectuer sur les données complètes.<br>\n",
    "\n",
    "3-Avant d'être envoyés à l'étape REDUCE, les résultats intermédiaires sont regroupés et triés par clé. C'est l'étape deSHUFFLE and SORT.<br>\n",
    "\n",
    "4-Enfin, l'étape REDUCE consiste à appliquer l'opération reduce, spécifiée pour notre problème, à chaque clé. Elle agrège tous les résultats intermédiaires associés à une même clé et renvoie donc pour chaque clé une valeur unique.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    <center> Architecture Map Reduce : </center> <br />\n",
    "    <img src=\"mapreduce.png\" alt=\"Architecture Map Reduce\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:justify;\">Spark élargit le cadre map/reduce en proposant à l'utilisateur des opérations supplémentaires pouvant être réalisées de manière distribuée. Spark emploie diverses techniques d'optimisation, mais au final tout calcul distribué est réalisé sous la forme d'opérations map/reduce. D'une certaine manière, MapReduce est le langage assembleur du calcul distribué : les outils permettant de réaliser des calculs distribués, tel Spark, permettent à l'utilisateur de s'abstraire de MapReduce ; tout comme les langages de programmation de haut niveau peuvent être compilés en assembleur mais permettent de ne pas avoir à écrire soi-même des programmes en assembleur.</p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Cluster Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un cluster Spark est composé de :\n",
    "\n",
    "- un ou plusieurs workers : chaque worker instancie un executor chargé d'exécuter les différentes tâches de calculs.\n",
    "- un driver : chargé de répartir les tâches sur les différents executors. C'est le driver qui exécute la méthodemainde nos applications.\n",
    "- un cluster manager : chargé d'instancier les différents workers.\n",
    "\n",
    "La différence entre worker et executor n'est pas évidente au premier coup d'œil. En pratique, on peut voir un worker comme une machine physique, et un executor comme une application qui tourne sur cette machine. Cette distinction permet d'exécuter plusieurs applications Spark sur une même machine en même temps. Chaque worker a alors plusieurs executors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    <center> Workers et Executors : </center> <br />\n",
    "    <img src=\"workers_&_executors.png\" alt=\"Workers et Executors\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'option --master permet de préciser à quel type de cluster manager l'application Spark peut être envoyée. Spark peut fonctionner en se connectant à des cluster managers de types différents :<br>\n",
    "\n",
    "--master spark://HOTE:PORT: utilise le cluster manager autonome de Spark. <br>\n",
    "--master mesos://HOTE:PORT: se connecte à un cluster manager Mesos. <br>\n",
    "--master yarn: se connecte à un cluster manager Yarn.<br>\n",
    "--master local: pas de cluster manager, Spark fonctionne en mode local. Il est possible de spécifier le nombre d'executors dans le cluster en passant une valeur entre crochets :local[1]oulocal[4], par exemple. <br>\n",
    "\n",
    "Par défaut, si l'option --master n'est pas spécifiée, Spark fonctionne en mode local avec un nombre d'executors égal au nombre de cœurs physique de la machine. Alors que si l'on spécifie--master local[1]un seul des quatre cœurs de la machine sera utilisé. <br>\n",
    "Le cluster manager est à ne pas confondre avec le driver. Le cluster manager est responsable de l'allocation des ressources, notamment lorsque plusieurs applications concurrentes sont exécutées sur le cluster Spark. Ce rôle d'allocation des ressources ne peut pas être confié au driver parce que le driver n'est responsable que de sa propre application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 - RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "un RDD (Resilient Distributed Dataset) est un framework Spark permettant de faire du calcul distribué.\n",
    "\n",
    "Les RDD possèdent deux types de méthodes :\n",
    "\n",
    "- Les transformations : qui donnent en sortie un autre RDD.\n",
    "- Les actions qui donnent en sortie... autre chose qu'un RDD.\n",
    "\n",
    "Pourquoi est-ce que cette distinction est importante ? Parce que les transformations ne sont pas évaluées immédiatement. On dit qu'elles sont évaluées de manière paresseuse (\"lazy evaluation\"), c'est à dire uniquement lorsqu'on en a besoin. Et on n'a besoin du résultat d'une transformation que lorsqu'on effectue une action. C'est pour cela que les appels à flatMap,map et reduceByKey se font de manière quasi instantanée. C'est parce que ces appels, en soit, ne font pas grand-chose. Les fonctions passées en argument de ces transformations ne sont appelées qu'au moment de l'appel àcollect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les principales transformations et actions disponibles dans Spark sont les suivantes :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    <center> Principales transformations et actions : </center> <br />\n",
    "    <img src=\"RDD.png\" alt=\"RDD\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 - Calculs distribués sous forme de graphe avec les DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si les RDD sont aussi importants dans Spark, c'est parce qu'ils dictent la manière dont les calculs vont être distribués sur les différentes machines. Ce sont aussi les RDD qui permettent une tolérance aux pannes, sujet épineux comme on a pu le voir en introduction de ce chapitre.\n",
    "\n",
    "Dans une application Spark, les transformations et les actions réalisées sur les RDD permettent de construire un graphe acyclique orienté (DAG : \"directed acyclic graph\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    <center> Calculs distribués sous forme de graphes avec les DAG : </center> <br />\n",
    "    <img src=\"dag.png\" alt=\"DAG\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette architecture DAG peut être visualisé lors de l'execution d'une application Spark en tapant localhost:4040/jobs/ dans un navigateur. Voici ci-dessous un exemple de visualisation de l'architecture DAG d'un job exécuté ci-dessus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    <center> Spark et DAG Visualisation : </center> <br />\n",
    "    <img src=\"nb.png\" alt=\"nb\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 - Fonctionnement de spark : cycle de vie d'une application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment Spark distribue les calculs sur les différents executors ? En particulier, comment les données sont-elles réparties entre les executors ?\n",
    "\n",
    "Les données sont découpées en partitions. Chaque partition est assignée à un des executors. Le traitement d'une partition représente une tâche : c'est la plus petite unité de traitement de données. Un cluster Spark ne peut traiter qu'une tâche à la fois par executor, et en général il y a un executor par cœur de processeur. Par ailleurs, la taille d'une partition doit rester inférieure à la mémoire disponible pour son executor. Le choix du nombre de partitions est donc crucial, puisqu'il détermine le nombre de tâches qui seront réalisées de manière concurrente sur le cluster. \n",
    "\n",
    "Un ensemble de tâches réalisées en parallèle, une par partition d'un RDD, constitue une étape (stage). Toutes les tâches d'une étape doivent être terminées avant que l'on puisse passer à l'étape suivante. Un job Spark est composé d'une succession d'étapes ; la progression d'un job peut donc être mesurée au nombre d'étapes qui ont été réalisées. Un job Spark est créé pour chaque action qu'on réalise sur un RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Job Spark : <br />\n",
    "    <img src=\"job_spark.png\" alt=\"Job Spark\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelles tâches regroupent-on dans une même étape ? On passe d'une étape à une autre dès lors qu'on doit redistribuer les données entre les nœuds. On dit alors qu'il y a un shuffle. Les shuffle peuvent se produire pour des raisons différentes. Par exemple, lors d'unreduceByKeytoutes les données correspondant à une même clé sont regroupées sur la même partition. Il est important de comprendre quelles actions nécessitent un shuffle car le transfert de données entre différentes machines (par le réseau) est coûteux en temps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Job Spark :<br />\n",
    "    <img src=\"job_spark_2.png\" alt=\"Job Spark\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Récapitulatif :\n",
    "- Un job Spark correspond à une action sur un RDD et est composé de plusieurs étapes séparées par des shuffles.\n",
    "- Chaque étape est composée de tâches.\n",
    "- Chaque tâche s'exécute sur une partition différente des données.\n",
    "- Les partitions sont réparties sur les différents executors.\n",
    "- Les partitions sont créées par les Resilient Distributed Datasets (RDD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour résumer, la distribution des calculs sur Spark intervient lorsque ce dernier décompose le traitement des opérations RDD en tâches, chacune étant exécutée par un exécuteur.Spark permet de partager les données en mémoire entre les graphes, de façon à ce que plusieurs jobs puissent travailler sur le même jeu de données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.6 - Distribution des calculs dans notre application Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaque SparkContext lance une interface utilisateur Web, par défaut sur le port 4040, qui affiche des informations utiles et des metrics relatifs à l'application. Ceci comprend:\n",
    "\n",
    "- Une liste des étapes et des tâches du planificateur\n",
    "- Un résumé des tailles RDD et de l'utilisation de la mémoire\n",
    "- Informations environnementales.\n",
    "- Informations sur les exécuteurs exécutants\n",
    "\n",
    "<p>\n",
    "    <center> Spark UI : </center><br />\n",
    "    <img src=\"spark.png\" alt=\"Spark UI\" />\n",
    "</p>\n",
    "<br>\n",
    "Elle permet également de visualiser l'achitecture en graphe des job (DAG Visualistion).\n",
    "<p>\n",
    "    <img src=\"job1.png\" alt=\"Spark UI\" />\n",
    "</p>\n",
    "<br>\n",
    "Lors de la création de sparkContext, je n'ai pas configuré le master, il a pris par défaut la valeur local (etoile), ce qui indique à Spark de créer autant de threads de travail que de coeurs logiques sur mon ordinateur. Nous pouvons ainsi constater grâce à l'interface web de Spark UI que les différents calculs sont distribués par Spark sur les 4 coeurs logiquesdont dispose mon ordinateur. <br>\n",
    "<p>\n",
    "    <img src=\"logic_2.png\" alt=\"Coeur logic Mac\" />\n",
    "</p>\n",
    "<br>\n",
    "Lorsqu'on clique sur un stage, on accède à différentes informations sur les métriques relatifs à ce stage. <br>\n",
    "<p>\n",
    "    <img src=\"metrics.png\" alt=\"Details metrics stages\" />\n",
    "</p>\n",
    "<br>\n",
    "En suivant le tutoriel suivant https://blog.ippon.fr/2014/11/20/utiliser-apache-spark-en-cluster/, j'ai tenté de visualiser l'interface web des clusters mais cela a échoué, pourtant ./start-master.sh et tail -f spark-winnievorihilala-org.apache.spark.deploy.master.Master-1-Air-de-Winnie.out créent bien un fichier de log et affiche dans le terminal les urls pour accéder à l'interface : \n",
    "\n",
    "20/03/16 11:18:40 INFO Utils: Successfully started service 'sparkMaster' on port 7077.\n",
    "\n",
    "20/03/16 11:18:40 INFO Master: Starting Spark master at spark://Air-de-Winnie:7077\n",
    "\n",
    "20/03/16 11:18:41 INFO MasterWebUI: Bound MasterWebUI to 0.0.0.0, and started at http://air-de-winnie:8080\n",
    "\n",
    "<p>\n",
    "    <img src=\"cluster.png\" alt=\"Job Spark\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sources :  \n",
    "- https://meritis.fr/challenge/bigdata/larchitecture-framework-spark/ <br>\n",
    "- https://spark.apache.org/docs/latest/api/python/index.html <br>\n",
    "- https://spark.apache.org/docs/latest/rdd-programming-guide.html <br>\n",
    "- https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives <br>\n",
    "- https://blog.jetoile.fr/2014/05/rdd-quest-ce-que-cest.html <br>\n",
    "- http://b3d.bdpedia.fr/spark-batch.html <br>\n",
    "- https://blog.zenika.com/2015/02/02/introduction-a-spark/ <br>\n",
    "- https://blog.ippon.fr/2014/11/20/utiliser-apache-spark-en-cluster/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
